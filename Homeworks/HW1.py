# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CD6UV7yP2ZFiSeZbQuxqDihbVWJDR_my
"""

1) How would you define Machine Learning?

Machine Learning is the field of study in which computer can be made to learn without being actually programmed.

2) What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these.

Supervised Learning: In this the computer is given the data. So there is a relationship between inout and output. This data is known as train dataset.
So based on this data, computer predicts the target variable for the given input value. supervised learning is divided in to Regression and classification
problem.

Ex: classifying emails as spam and non-spam is a classification problem.

Unsupervised Learning: In this no input output relationship is given. We dont know the effect of variables. So we can build structure of data by 
clustering on basis of relationship between variable values.

Ex: classify music and vocals for mixture of both.

3) What are the test and validation set, and why would you want to use them?

Without validation and test set we can not use model for real life problems. So after traning the model we use validation set to ensure correctness of
the model. This set is used in data preperation step. While Test set is used in the final step after which model can be used for real problems. 
Test set is used to compare performance of different models.

4) What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?

1. Duplicate values: Duplicate values can lead to bias for certain data object. So removing duplicate values is must.

2. Imbalanced Data: If number of classes are different for different variables than it is called imbalanced dataset. The data which has less classes 
is oversampled or data with more classes is undersampled to make data set equal.

3. Missing values: The missing values are droped or can be filled by mean values in case normal distrbution and median values in other cases.

4. Outlier Detection: They are the data set values which dont fit in the model in some way. They are removed while traning the algorithm. Interquartile
range is used to detect outliers.

5. feature scaling: It is the process to normalise the data set for the uniformity. It can be done by standardization or normalisation.

6. Bucketing: This is used to reduce small observation data errors. Whole data set is dived in to small bins. Each bin is assigned a value based on the
data values inside that bin.

7. Feature Extraction: It is the process in which the unrelated(unwanted) variables are removed from the data set. This will help in easy processing of
data set.

8. Feature Encoding: In this the data is converted in to numeric form which can be processed by the algorithm. This can be don by either nominal method 
or ordinal method. In nominal method one-hot technique is used.

9. Train or Validation or Test Split: The data set can be split in to 2 or 3 data sets. Traiin data set is used for traning the model.
After traning the model we use validation set to ensure correctness of the model. This set is used in data preperation step. While Test 
set is used in the final step after which model can be used for real problems.  Test set is used to compare performance of different models.

10. cross validation: In this the data set is split in to mutiple sets. In each round different data set in picked as a validation set. This is known 
as cross validation.

5) How you can explore continuos and discrete variables?

We can explore continuous and discrete variables by the graphical representation of the data set provided.

6) Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it.)

The variable given in the plot is continuous. 

In preprocessing period, the data can be averaged on basis of its standard deviation. The missing values in the graph can be filled by the median of 
the given data.